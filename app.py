import streamlit as st

# –¢—É—Ç –º–∏ —ñ–º—ñ—Ç—É—î–º–æ –≤–∏–∫–ª–∏–∫ AI –º–æ–¥–µ–ª—ñ.
# –í —Ä–µ–∞–ª—å–Ω–æ–º—É –∂–∏—Ç—Ç—ñ, —Ç—É—Ç –±—É–≤ –±–∏ –≤–∏–∫–ª–∏–∫ –¥–æ OpenAI, Gemini, HuggingFace —á–∏ —ñ–Ω—à–æ–≥–æ API.
def get_model_response(prompt: str) -> dict:
    """
    –Ü–º—ñ—Ç—É—î –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å.
    –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó, —è–∫—â–æ prompt –º—ñ—Å—Ç–∏—Ç—å "image" ‚Äì –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —ñ–Ω–∞–∫—à–µ ‚Äì —Ç–µ–∫—Å—Ç.
    """
    st.info(f"AI Model called with prompt: '{prompt}'") # QA-–ª–æ–≥—ñ–Ω–≥ —É –∫–æ–Ω—Å–æ–ª—å!

    if "image" in prompt.lower():
        # –Ü–º—ñ—Ç–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, —â–æ –º—ñ—Å—Ç–∏—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤—ñ–¥ DALL-E)
        return {
            "type": "image",
            "content": "https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_example.png",
            "metadata": {"source": "DALL-E-like model"}
        }
    elif "error" in prompt.lower():
        # –Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–º–∏–ª–∫–∏ –≤—ñ–¥ –º–æ–¥–µ–ª—ñ
        raise ValueError("Model failed to process the request due to internal server error.")
    else:
        # –Ü–º—ñ—Ç–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç–æ–≤–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤—ñ–¥ GPT)
        return {
            "type": "text",
            "content": f"–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ñ–¥ –º–æ–¥–µ–ª—ñ –¥–ª—è –∑–∞–ø–∏—Ç—É: '{prompt}'. –ú–æ—ó –≤—ñ—Ç–∞–Ω–Ω—è, —Ü–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å.",
            "metadata": {"source": "LLM-like model"}
        }

## --- Streamlit UI Section ---

st.title("ü§ñ QA's Prompt-to-Result Service")
st.caption("–í–≤–µ–¥–∏ –ø—Ä–æ–º–ø—Ç, —ñ –ø–æ–±–∞—á–∏—à –º–∞–≥—ñ—é AI (–∞–±–æ QA üòâ)")

# –§–æ—Ä–º–∞ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç–∞
with st.form(key='prompt_form'):
    user_prompt = st.text_area("–í–∞—à Prompt —Ç—É—Ç:", height=100, key="prompt_input")
    submit_button = st.form_submit_button(label='–û—Ç—Ä–∏–º–∞—Ç–∏ –†–µ–∑—É–ª—å—Ç–∞—Ç')

# –û–±—Ä–æ–±–∫–∞ –Ω–∞—Ç–∏—Å–∫–∞–Ω–Ω—è –∫–Ω–æ–ø–∫–∏
if submit_button and user_prompt:
    st.subheader("‚úÖ AI Response:")
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ st.spinner –¥–ª—è –∫—Ä–∞—â–æ–≥–æ UX (User Experience)
    with st.spinner('–ß–µ–∫–∞—î–º–æ –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –º–æ–¥–µ–ª—ñ... (–¶–µ –Ω–∞–π—Å–∫–ª–∞–¥–Ω—ñ—à–∏–π Test Case - –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è!)'):
        try:
            # –í–∏–∫–ª–∏–∫ —ñ–º—ñ—Ç–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ
            response_data = get_model_response(user_prompt)
            
            # --- –õ–æ–≥—ñ–∫–∞ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç—É ---
            
            response_type = response_data.get("type")
            content = response_data.get("content")
            metadata = response_data.get("metadata", {})
            
            if response_type == "text":
                st.success("–û—Ç—Ä–∏–º–∞–Ω–æ —Ç–µ–∫—Å—Ç:")
                st.markdown(f"**{content}**")
                
            elif response_type == "image":
                st.success("–û—Ç—Ä–∏–º–∞–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è:")
                st.image(content, caption=f"–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–ª—é: {metadata.get('source')}")
                
            else:
                st.error("–ü–æ–º–∏–ª–∫–∞ QA: –ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ –º–æ–¥–µ–ª—ñ!")
                st.json(response_data) # –ü–æ–∫–∞–∑—É—î–º–æ —Å–∏—Ä–∏–π JSON –¥–ª—è Debugging
                
            st.markdown("---")
            st.code(f"Metadata: {metadata}", language="json")

        except ValueError as e:
            # –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ (Error Handling)
            st.error(f"‚ùå Model Error: {e}")
            st.warning("–¶–µ —Ç–µ, —â–æ –º–∏, QA, –ª—é–±–∏–º–æ –Ω–∞–π–±—ñ–ª—å—à–µ ‚Äì —Å–ø—Ä–∞–≤–∂–Ω—ñ–π –±–∞–≥! –¢—Ä–µ–±–∞ –π–æ–≥–æ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç—É–≤–∞—Ç–∏.")

elif submit_button:
    st.warning("–ù–µ –∑–∞–±—É–¥—å—Ç–µ –≤–≤–µ—Å—Ç–∏ Prompt! –ú–æ–¥–µ–ª—å –Ω–µ –≤–º—ñ—î —á–∏—Ç–∞—Ç–∏ –≤–∞—à—ñ –¥—É–º–∫–∏ (–ø–æ–∫–∏ —â–æ).")